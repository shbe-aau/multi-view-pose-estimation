import torch
from DatasetGeneratorOpenGL import DatasetGenerator
from Pipeline import Pipeline
from Encoder import Encoder
from Model import Model
import json
import pickle
import numpy as np
import matplotlib.pyplot as plt
from utils.tools import *
from utils.utils import *
from scipy.spatial.transform import Rotation

def pointToMat(point):
    r = Rotation.from_euler('yz', point['spherical']) # select point wanted for comparison here
    R = r.as_matrix()
    return torch.from_numpy(R)

def main():
    device = torch.device("cuda:0")
    num_datapoints = 1984
    views = 10
    load_points = True
    base_directory = './output/paper-models/10views/obj10/visualization/'
    prepareDir(base_directory)
    # Need to replace sundermeyer-random with something where we can
    # use predetermined poses, to plot each arch to visualize
    datagen = DatasetGenerator("",
                            "./data/cad-files/ply-files/obj_10.ply",
                            375,
                            num_datapoints,
                            "not_used",
                            device,
                            "sundermeyer-random",
                            random_light=False)

    # use fixed data instead
    # data = loadDataset(['./data/validationsets/tless-train-obj10.p'], 8)

    # load model
    encoder = Encoder("./data/obj1-18/encoder.npy").to(device)
    encoder.eval()
    #checkpoint = torch.load("./output/depth/multi-path-reconst/depth-reconst/obj10-10views/models/model-epoch200.pt") # Change this later
    checkpoint = torch.load("./output/paper-models/10views/obj10/models/model-epoch199.pt")

    model = Model(num_views=views).cuda()
    model.load_state_dict(checkpoint['model'])
    model = model.eval()
    pipeline = Pipeline(encoder, model, device)

    # run images through model
    # Predict poses
    # around x
    shiftx = np.eye(3, dtype=np.float)
    theta = np.pi / num_datapoints
    #theta = np.pi / 3
    shiftx[1,1] = np.cos(theta)
    shiftx[1,2] = -np.sin(theta)
    shiftx[2,2] = np.cos(theta)
    shiftx[2,1] = np.sin(theta)
    # around y
    shifty = np.eye(3, dtype=np.float)
    theta = np.pi / num_datapoints
    shifty[0,0] = np.cos(theta)
    shifty[0,2] = -np.sin(theta)
    shifty[2,2] = np.cos(theta)
    shifty[2,0] = np.sin(theta)
    # around z
    shiftz = np.eye(3, dtype=np.float)
    theta = np.pi / num_datapoints
    shiftz[0,0] = np.cos(theta)
    shiftz[0,1] = -np.sin(theta)
    shiftz[1,1] = np.cos(theta)
    shiftz[1,0] = np.sin(theta)
    predicted_poses = []
    predicted_poses_raw = []
    R_conv = np.eye(3, dtype=np.float)
    #R_conv = np.array([[ 0.5435,  0.1365,  0.8283],
    #                   [ 0.6597,  0.5406, -0.5220],
    #                   [-0.5190,  0.8301,  0.2037]])
    #R_conv = np.array([[-0.7132,  0.0407,  0.6998],
    #                   [ 0.1696, -0.9586,  0.2287],
    #                   [ 0.6802,  0.2818,  0.6767]])
    #R_conv = np.array([[-0.9959,  0.0797,  0.0423],
    #                   [ 0.0444,  0.0249,  0.9987],
    #                   [ 0.0786,  0.9965, -0.0283]])

    if load_points:
        # Try with points from the sphere
        points = np.load('./output/depth/spherical_mapping_obj10_1_500/points.npy', allow_pickle=True)
        num_datapoints = len(points)

        Rin = []
        for point in points:
            Rin.append(pointToMat(point))
            #Rin.append(np.matmul(pointToMat(point), shiftx))

    else:
        Rin = []
        for i in range(num_datapoints):
            # get data from fixed R and T vectors
            R_conv = np.matmul(R_conv, shiftx)
            R = torch.from_numpy(R_conv)
            Rin.append(R)

    t = torch.tensor([0.0, 0.0, 375])
    data = datagen.generate_image_batch(Rin = Rin, tin = t, augment = False)

    output = pipeline.process(data["images"])

    # evaluate how output confidence and each view changes with input pose
    plot_confidences(output.detach().cpu().numpy())
    plot_flat_landscape(points, output[:,0:views].detach().cpu().numpy())

    rotation_matrices = []
    for i in range(views):
        start = views + i*6
        end = views + (i + 1)*6
        curr_poses = output[:,start:end]
        matrices = compute_rotation_matrix_from_ortho6d(curr_poses)
        euler_angles = compute_euler_angles_from_rotation_matrices(matrices)
        print(matrices.shape)
        print(matrices[0:3])
        print(euler_angles.shape)
        print(euler_angles[0:3])
        exit()

def plot_confidences(predicted_poses):
    num_datapoints = len(predicted_poses)
    views = int(len(predicted_poses[0])/7)

    x = np.linspace(0, np.pi, num_datapoints)
    #fig, ax = plt.subplots(int(views/2),2)
    fig = plt.figure()
    fig.set_size_inches(15, 10)
    fig.tight_layout(rect=(0,0,0.95,1),pad=1.0)
    for i in range(views):
        #fig.add_subplot(int(views/2),2,i+1)
        plt.plot(x,predicted_poses[:,i], label=i)
        plt.ylim([-0.01, 1.01])
        plt.xlim([-0.01, 1.01])

    plt.legend(loc="upper right")
    # plt.show()
    fig.savefig('./output/paper-models/10views/obj10/visualization/confidence.png', bbox_inches='tight')

def plot_flat_landscape(points_in, conficences):
    angles = [point['spherical'] for point in points_in]
    # shift for clearer image
    shift_radians = 1.1
    for i in range(len(angles)):
        temp = (angles[i][1] + shift_radians)
        angles[i][1] = temp if temp < 2*np.pi else temp-2*np.pi
    theta, phi = zip(*angles)

    from scipy.spatial import Voronoi, voronoi_plot_2d

    #print(angles[0])
    angles = np.array(angles)
    angles[:,[0, 1]] = angles[:,[1, 0]]
    #print(angles[0])
    vor = Voronoi(angles)

    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

    # plot Voronoi diagram, and fill finite regions with color mapped from losses
    fig = voronoi_plot_2d(vor, show_points=False, show_vertices=False, line_alpha=0, s=1)
    ax = fig.add_subplot(1,1,1)
    fig.set_size_inches(45, 15)
    fig.tight_layout(rect=(0,0,0.95,1),pad=1.0)
    ax.grid(False)
    ax.set_xticks([])
    ax.set_yticks([])
    #plt.axis('off')
    plt.ylim([min(theta), max(theta)])
    plt.xlim([min(phi), max(phi)])
    print(len(vor.point_region))
    for r in range(len(vor.point_region)):
        region = vor.regions[vor.point_region[r]]
        if not -1 in region:
            polygon = [vor.vertices[i] for i in region]
            view = np.argmin(conficences[r,:])
            plt.fill(*zip(*polygon), color=colors[view])

    fig.savefig('./output/paper-models/10views/obj10/visualization/confidence_landscape.png', bbox_inches='tight', dpi=fig.dpi)

# Copied from train.py for now
def loadDataset(file_list, batch_size=2):
    #data = {"codes":[],"Rs":[],"images":[]}
    data = []
    for f in file_list:
        print("Loading dataset: {0}".format(f))
        with open(f, "rb") as f:
            curr_data = pickle.load(f, encoding="latin1")
            curr_batch = {"Rs":[],"images":[]}
            for i in range(len(curr_data["Rs"])):
                curr_pose = curr_data["Rs"][i]

                # Convert from T-LESS to Pytorch3D format
                xy_flip = np.eye(3, dtype=np.float)
                xy_flip[0,0] = -1.0
                xy_flip[1,1] = -1.0
                curr_pose = np.transpose(curr_pose)
                curr_pose = np.dot(curr_pose, xy_flip)
                curr_batch["Rs"].append(curr_pose)

                # Normalize image
                curr_image = curr_data["images"][i]
                curr_image = curr_image/np.max(curr_image)
                curr_batch["images"].append(curr_image)

                if(len(curr_batch["Rs"]) >= batch_size):
                    data.append(curr_batch)
                    curr_batch = {"Rs":[],"images":[]}
            data.append(curr_batch)
    return data

if __name__ == '__main__':
    main()
